---
title: "Vignette Title"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Overview

This vignette demonstrates how to analyze cell-profiler acquired data 
of 96-well plates, treated with different conditions. 

# Update 
To ensure that the latest package version is installed, we will fetch it 
from github.

```{r, eval=F}
# TODO: Test for devtools
if (!"devtools" %in% installed.packages()) {
    install.packages("devtools")
}
devtools::install_github("mknoll/cmoRe")
```

```{r}
require(cmoRe)
```

# Data requirements

This analysis pipeline was built to analyze data obtained with an 
InCell System, and analyzed using Cell Profiler. The cell profiler pipeline
is published in X. 

A typical experiment might be structured as follows:

96 Well plate in which cells (from one batch) are seeded.
Treatment with n different conditions, 3 replicates (=wells) per condition.
Three repetitions of the experiment. 

For each run, two files created by CellProfiler and a treatment file are
needed. 

The CellProfiler files are named as follows:
Cells.txt: Cell measurements
Primarieswithoutborder.txt: Nuclear measurements

When merging both files by object id, similar variable names will be changes 
to var.x for cell derived data and var.y for nuclear data. 

```{r}
read.csv(system.file("extdata", "run1_1/Cells.txt", package = "cmoRe"), sep="\t")[1:3,1:5]
```

Additionally, a treatment file is required:
Treatment.csv 
containing at least two columns, "well" and "Treatment".
Further columns might be supplied, e.g. a "Konzentration" Column. 

```{r}
read.csv(system.file("extdata", "run1_1/Treatment.csv", package = "cmoRe"))[1:5,]
```

All three files should be stored in a single folder for each experimental run.

# Data structure

Multiple 96 well plates might be part of an experiment. 
Each experimental run is encoded as list element, again containing a list 
of all folders wich are part of the respective run.

A design with 2 independent runs with 2 plates each might look as follows:

```{r}
data <- list()
data[[length(data)+1]] <- list(system.file("extdata", "run1_1/", package = "cmoRe"),
                               system.file("extdata", "run1_2/", package = "cmoRe"))
data[[length(data)+1]] <- list(system.file("extdata", "run2_1/", package = "cmoRe"),
                               system.file("extdata", "run2_2/", package = "cmoRe"))
```

# Experiment ID 

Each experiment is associated with an identifier.
```{r}
uid <- "testexp"
```

# Create experiment

Now, we can create the imageExp instance, in which the data will be stored.

```{r}
exp <- new("imageExp", uid, data)
```

The next step is loading of the data. If only a subset of variables should be loaded,
a vars parameter might be supplied. In this case, we only use two of the cell profiler 
output files and have to specify this, default is three files:

- Cells.txt: Total cell data
- Primarieswithoutborder.txt: Usually nuclear data

In third file is 

- Cytoplasm.txt: Cytoplasmatic data

```{r}
exp <- getData(exp, 
	       files=c("Treatment.csv", "Cells.txt", "Primarieswithoutborder.txt"))

#only read a single variable
#exp <- getData(exp, 
#	       files=c("Treatment.csv", "Cells.txt", "Primarieswithoutborder.txt"),
#		vars="AreaShape_Area")
```

# QC 

To get a first impression, all variables and the number of 
cells per well can be plotted side by side to a treatment plan. Results are stored
as pdf file, an output folder and filename can be specified.
```{r}
## TODO
#exp <- checkFilter(exp, filter=100)

# number of cells
exp <- qcPlots(exp, folder="/tmp/") 
exp <- qcPlots(exp) 

# Cell-Size
exp <- qcPlots(exp, vars=c("AreaShape_Area.cell"), folder="/tmp/")
```

# Cell-cycle and Cell/Nuclear Ratio

Nuclear DNA Intensity and Cell/Nuclear Size can be used to determine cell cyclus
for each cell and filter certain cell populations.

Calculation of Cellcycle is performed by supplying fun="cc", ratios with fun="nc".
Not specifying the fun parameter will calculate both.

This step might be time consuming, depending on the number of bootstraps to obtain
cutoffs.
```{r}
exp <- calc(exp, fun="nc")
```

# Cell cycle 
We can plot cell cycles per plate or aggregated per run to evalaute quaality.
Data is not included in the supplied data.
```{r eval=F}
#exp <- calc(exp, fun="cc")
analyzeCC(exp)
##per plate
#analyzeCC(exp, aggregate=T)
```

# Filter data 

Next, data is filtered by the previously calculated cutoffs to remove dead cells.

```{r}
exp <- filter(exp)
### TODO
#exp <- checkFilter(exp, filter=100)
```

# Additional variables (diff, ratio)

Differences and ratios between Cell/Nuclear variables can be easily added using 
the addVars function.
```{r}
exp <- addVars(exp, c("AreaShape_Area"))
colnames(exp@data)[which(grepl("AreaShape", colnames(exp@data)))]
```

# Aggregation per well

For the following analyses, all measurements are aggregated per well (default: median),
might be specified by the fun argument. 

```{r}
exp <- agg(exp)
```

# Remove raw data

As exp object might become quite large, we only want to store derived data. Therefore,
we remove the raw data and can then save the resulting exp object. Not performed here,
as we need the raw data for single cell analysis (see below).

```{r}
#exp <- removeRawData(exp)
```

# Processing log
To keep track of which processing steps were performed, we can inspect the log of 
each element.

```{r}
getLog(exp)
```

# Z-transform

We can normalize the data by z-transforming data per run. Similar treated wells are required
in all runs. 

```{r}
exp <- zTrans(exp)
```

# Extract data 
```{r}
#non-transformed data
exp@dataAgg[1:3,]
#z-transformed data
exp@dataZ[1:3,]
```

# Analyze I

Differentially regulated variables

```{r}
dat <- exp@dataZ
tmp <- do.call(rbind, strsplit(as.character(dat$TREATMENT), "_"))

dat$GRP <- tmp[,1]
## ggf log transformieren
## ggf zentrieren der dosis 
dat$DOSE <- tmp[,2] 
dat$DOSE[which(tmp[,2] == 1)] <- 3
dat$DOSE[which(tmp[,2] == 2)] <- 2
dat$DOSE[which(tmp[,2] == 3)] <- 1
dat$DOSE[which(tmp[,1] == "Ko")] <- 0

table(dat$DOSE, dat$VERSUCH, dat$GRP)

###lme4
frm <- as.formula(VAL~DOSE+(1|VERSUCH/PLATTE/TREATMENT)) 
frm0 <- as.formula(VAL~1+(1|VERSUCH/PLATTE/TREATMENT))  
res <- analyze(dat, ref="Ko_1", frm=frm, frm0=frm0)

###nlme
#frm <- as.formula(VAL~DOSE) 
#frm0 <- as.formula(VAL~1)
#rand <- as.formula(~1|VERSUCH/PLATTE/TREATMENT)
#res <- analyzeNLME(dat, ref="Ko_1", frm=frm, frm0=frm0, rand=rand, level=0)
```

# Filter results 
Keep only models which are significantly better compared to the respective 
null model, e.g. after Benjamini-Hochberg p-value adjustment

```{r}
lapply(res, function(x) { 
           ## add p.value, usign a z-distribution 
           x$p.value <- 2*(1-pnorm(abs(x$t.value)))
           ## remove all vars which have an R2cv < 0.9
           x <- x[which(x$R2cv > 0.9),]
           ## selektiere einzelne modelle
           tmp <- x[which(!duplicated(x$VAR)),]
           tmp$padj <- p.adjust(tmp$aP, "BH")
           x$padj <- tmp$padj[match(x$VAR, tmp$VAR)]
           ## we apply only weak filters here
           x <- x[which(x$padj < 0.2),]
           x <- x[which(x$p.value < 0.2),]
           x
    })
```


# Remove unwanted variation
```{r}
require(lme4)
frm <- as.formula(VAL~1+(1|VERSUCH/PLATTE/TREATMENT)) 
dat <- exp@dataZ
dat <- dat[,which(!apply(dat, 2, function(x) all(is.na(x))))]

#remove metadata
cn <- colnames(dat[,-which(colnames(dat) %in% c("TREATMENT", "VERSUCH", "PLATTE","WELL"))])

coll <- list()
for (i in 1:length(cn)) {
    dat$VAL <-dat[,which(colnames(dat) == cn)[i]]
    fit <- lmer(frm, data=dat[,])
    coll[[length(coll)+1]] <- predict(fit)
    names(coll)[length(coll)] <- cn[i]
}
coll <- do.call(cbind, coll)

require(pheatmap)
image(t(dat[,cn]))
s <- apply(coll, 1, function(x) paste(x, collapse=";"))
image(t(coll[which(!duplicated(s)),cn]))
```



# Identify clusters
General approach for cluster identification, usually using prefiltered data (see above).
For demonstration purposes, all data is used. For demonstration, z transformed data is used.

```{r}
require(ggplot2)
################
### select data ##
##################
#dat <- exp@dataZ[,selVars]
dat <- exp@dataZ
#remove metadata
dat <- dat[,-which(colnames(dat) %in% c("TREATMENT", "VERSUCH", "PLATTE","WELL"))]
###############
#remove only NA cols
dat <- dat[,-which(apply(dat, 2, function(x) all(is.na(x))))]

######################
###### calculate cluster variability
######################
cluster <- findClusters(dat, nClust = c(1,50))

df <- data.frame(apply(cluster[,1:2], 2, unlist))
cutoff <- 2
ggplot(df, aes(y=dispSum, x=i))+ geom_point() + geom_line() + labs(x="#clusters", y="variability") + geom_hline(yintercept=cutoff)

#####################
## Calculate synthetic variables
######################
cm <- cluster
cm <- cm[order(unlist(cm[,"dispSum"])),]
map <- cm[which(cm[,"dispSum"] > cutoff),,drop=F][1,,drop=F][1,"pm.clust"][[1]]
data <- calculateSyntheticVars(dat, map)


###################
## Radarplots #####
##################
synthMetr <- calculateSyntheticVars(dat, map)
synthMetrDat <- cbind(synthMetr$data, exp@dataZ)

col <- list(c(rgb(0.1,0.1,0.1,0.5), rgb(0.1,0.5,0.5,0.5)), 
	    c(rgb(0.1,0.1,0.1,0.5), rgb(0.9,0.4,0.4,0.5)), 
	    c(rgb(0.1,0.1,0.1,0.5), rgb(0.9,0.9,0.5,0.5)),
	    c(rgb(0.1,0.1,0.1,0.5), rgb(0,100/255,0,0.5)))
par(mfrow=c(1,3), mar=c(0,0,2,0))
drawRadarplots(synthMetrDat, vars=names(synthMetr$anno), labels = F, ctrlLevel="Ko_1", pTest="t.test", agg=median,col=col) 
```
